{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179fae1d-4cc7-4f86-95f3-33a3de1b63b5",
   "metadata": {},
   "source": [
    "# Problem Set 9: PCA and the QR Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd3706-5e49-49e1-84d3-d0fefb5da00b",
   "metadata": {},
   "source": [
    "## Part I: Principal Component Analysis (PCA)\n",
    "\n",
    "### Problem 1: Explained Variance and Dimensionality Reduction\n",
    "\n",
    "A researcher performs PCA on a dataset with 8 features. The eigenvalues of the covariance matrix are:\n",
    "\n",
    "$$\n",
    "    \\lambda_1 = 42.5, \\quad \\lambda_2 = 18.3, \\quad \\lambda_3 = 9.7, \\quad \\lambda_4 = 5.2, \\lambda_5 = 2.8, \\quad \\lambda_6 = 1.5, \\quad \\lambda_7 = 0.6, \\quad \\lambda_8 = 0.4\n",
    "$$\n",
    "\n",
    "**(a)** Compute the total variance in the original data.\n",
    "\n",
    "**(b)** Calculate the percentage of variance explained by each principal component.\n",
    "\n",
    "**(c)** How many principal components are needed to retain at least 90% of the variance? At least 95%?\n",
    "\n",
    "**(d)** If you reduce the data to 3 dimensions, what is the reconstruction error as a percentage of total variance?\n",
    "\n",
    "### Problem 2: PCA Implementation and Reconstruction (Coding)\n",
    "\n",
    "Generate a synthetic dataset with correlation structure as follows:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "n_samples = 800\n",
    "# True latent factors\n",
    "Z = np.random.randn(n_samples, 2)\n",
    "# Mixing matrix to create 6 observed features\n",
    "A = np.array([\n",
    "    [1.5, 0.5],\n",
    "    [1.2, -0.8],\n",
    "    [0.9, 1.1],\n",
    "    [1.8, 0.3],\n",
    "    [0.7, -1.2],\n",
    "    [1.4, 0.9]\n",
    "])\n",
    "X = Z @ A.T + 0.5 * np.random.randn(n_samples, 6)\n",
    "```\n",
    "\n",
    "**(a)** Implement PCA from scratch (center data, compute covariance matrix, find eigenvalues/eigenvectors).\n",
    "\n",
    "**(b)** Create a scree plot showing explained variance vs. component number.\n",
    "\n",
    "**(c)** Project the data onto the first 2 principal components and visualize in 2D.\n",
    "\n",
    "**(d)** Reconstruct the original data using only the first $k$ components for $k = 1, 2, 3, 4$. For each $k$, compute the mean squared reconstruction error:\n",
    "\n",
    "$$\n",
    "    \\text{MSE}_k = \\frac{1}{n \\cdot d}\\|\\mathbf{X} - \\mathbf{X}_{\\text{reconstructed}}\\|_F^2\n",
    "$$\n",
    "\n",
    "### Problem 3: Data Whitening and Mahalanobis Distance (Coding)\n",
    "\n",
    "The whitening (or sphering) transformation decorrelates data and scales it to unit variance. Given centered data $\\mathbf{X}$ with eigendecomposition $\\mathbf{C} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^T$, the whitening transformation is:\n",
    "\n",
    "$$\n",
    "    \\mathbf{Z} = \\mathbf{X}\\mathbf{V}\\mathbf{\\Lambda}^{-1/2}\n",
    "$$\n",
    "\n",
    "**(a)** Explain why the covariance matrix of $\\mathbf{Z}$ is the identity matrix $\\mathbf{I}$.\n",
    "\n",
    "**(b)** Write a Python function `whiten(X)` that returns the whitened data matrix.\n",
    "\n",
    "**(c)** Generate test data with correlations:\n",
    "\n",
    "```python\n",
    "mean = [0, 0]\n",
    "cov = [[4, 2.4], [2.4, 2]]\n",
    "X = np.random.multivariate_normal(mean, cov, 500)\n",
    "```\n",
    "\n",
    "**(d)** Apply your whitening function and verify that `np.cov(Z.T)` is approximately the identity.\n",
    "\n",
    "**(e)** Plot both the original and whitened data. What geometric transformation has occurred?\n",
    "\n",
    "### Problem 4: Kernel PCA Concept\n",
    "\n",
    "Standard PCA finds linear combinations of features. Sometimes nonlinear structure exists in data.\n",
    "\n",
    "**(a)** Explain conceptually why applying PCA to $[\\mathbf{X}, \\mathbf{X}^2]$ (original features plus their squares) could capture nonlinear patterns.\n",
    "\n",
    "**(b)** Given 2D data that lies on a circle, would standard PCA find a useful 1D representation? Why or why not?\n",
    "\n",
    "**(c)** If we map data to a higher-dimensional space via $\\phi(\\mathbf{x})$ and then apply PCA in that space, we get kernel PCA. The key insight is that we never explicitly compute $\\phi(\\mathbf{x})$ but only compute inner products $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^T\\phi(\\mathbf{x}_j)$. For the polynomial kernel $K(\\mathbf{x}, \\mathbf{y}) = (1 + \\mathbf{x}^T\\mathbf{y})^2$, what is the dimensionality of the implicit feature space for 2D input data?\n",
    "\n",
    "## Part II: QR Algorithm for Eigenvalue Computation\n",
    "\n",
    "### Problem 5: QR Algorithm Convergence Analysis\n",
    "\n",
    "Consider the QR algorithm applied to a symmetric matrix with distinct eigenvalues $\\lambda_1 > \\lambda_2 > \\lambda_3 > 0$.\n",
    "\n",
    "**(a)** The QR algorithm generates a sequence $\\mathbf{A}_0, \\mathbf{A}_1, \\mathbf{A}_2, \\ldots$ where:\n",
    "\n",
    "$$\n",
    "    \\mathbf{A}_k = \\mathbf{Q}_k\\mathbf{R}_k \\quad \\text{and} \\quad \\mathbf{A}_{k+1} = \\mathbf{R}_k\\mathbf{Q}_k\n",
    "$$\n",
    "\n",
    "Prove that $\\mathbf{A}_{k+1}$ and $\\mathbf{A}_k$ are similar matrices (have the same eigenvalues).\n",
    "\n",
    "**(b)** The rate at which off-diagonal elements converge to zero is approximately geometric with rate $|\\lambda_3/\\lambda_2|$ for the $(2,3)$ entry and $|\\lambda_2/\\lambda_1|$ for the $(1,2)$ entry. If $\\lambda_1 = 10$, $\\lambda_2 = 5$, $\\lambda_3 = 0.5$, which off-diagonal element converges faster?\n",
    "\n",
    "**(c)** What pathological case prevents convergence of the basic QR algorithm? (Hint: consider eigenvalues with equal magnitude.)\n",
    "\n",
    "### Problem 6: QR Algorithm Implementation and Monitoring (Coding)\n",
    "\n",
    "Implement the basic QR algorithm and track its convergence behavior.\n",
    "\n",
    "```python\n",
    "A = np.array([\n",
    "    [4, 1, 1],\n",
    "    [1, 3, 0],\n",
    "    [1, 0, 2]\n",
    "], dtype=float)\n",
    "```\n",
    "\n",
    "**(a)** Write a function that performs $N$ iterations of the QR algorithm, storing $\\mathbf{A}_k$ at each iteration.\n",
    "\n",
    "**(b)** For each iteration, compute the off-diagonal norm:\n",
    "\n",
    "$$\n",
    "    \\text{off}(\\mathbf{A}_k) = \\sqrt{\\sum_{i \\neq j} A_k[i,j]^2}\n",
    "$$\n",
    "\n",
    "Plot this quantity versus iteration number on a log scale.\n",
    "\n",
    "**(c)** Extract the diagonal of $\\mathbf{A}_{100}$ and compare with eigenvalues from `np.linalg.eigvals(A)`.\n",
    "\n",
    "**(d)** Compute the theoretical convergence rate ratios $|\\lambda_i/\\lambda_{i-1}|$ and compare with your observed convergence plot.\n",
    "\n",
    "### Problem 7: Shifted QR Algorithm (Coding)\n",
    "\n",
    "The **shifted QR algorithm** dramatically improves convergence by incorporating a shift parameter $\\mu_k$ at each iteration. Here's how it works:\n",
    "\n",
    "**Standard QR Iteration:**\n",
    "1. Factor: $\\mathbf{A}_k = \\mathbf{Q}_k\\mathbf{R}_k$\n",
    "2. Recombine: $\\mathbf{A}_{k+1} = \\mathbf{R}_k\\mathbf{Q}_k$\n",
    "\n",
    "**Shifted QR Iteration:**\n",
    "1. Choose shift: $\\mu_k$ (various strategies exist)\n",
    "2. Factor the shifted matrix: $\\mathbf{A}_k - \\mu_k\\mathbf{I} = \\mathbf{Q}_k\\mathbf{R}_k$\n",
    "3. Recombine and add shift back: $\\mathbf{A}_{k+1} = \\mathbf{R}_k\\mathbf{Q}_k + \\mu_k\\mathbf{I}$\n",
    "\n",
    "Notice that $\\mathbf{A}_{k+1} = \\mathbf{Q}_k^T\\mathbf{A}_k\\mathbf{Q}_k$, so similarity is preserved.\n",
    "\n",
    "**Common shift strategies:**\n",
    "- **Rayleigh quotient shift**: $\\mu_k = \\mathbf{A}_k[n,n]$ (bottom-right entry)\n",
    "- **Wilkinson shift**: More sophisticated, based on the bottom-right $2 \\times 2$ block\n",
    "\n",
    "**(a)** Prove that the shifted iteration preserves similarity: show that $\\mathbf{A}_{k+1}$ and $\\mathbf{A}_k$ have the same eigenvalues.\n",
    "\n",
    "**(b)** Implement both unshifted and Rayleigh-quotient-shifted QR algorithms.\n",
    "\n",
    "**(c)** Test on the matrix:\n",
    "\n",
    "```python\n",
    "B = np.array([\n",
    "    [5, 2, 1, 0],\n",
    "    [2, 6, 2, 1],\n",
    "    [1, 2, 7, 2],\n",
    "    [0, 1, 2, 8]\n",
    "], dtype=float)\n",
    "```\n",
    "\n",
    "**(d)** For both algorithms, plot off-diagonal norm vs. iteration (run 50 iterations). How much faster does the shifted version converge?\n",
    "\n",
    "### Problem 8: Hessenberg Reduction Complexity\n",
    "\n",
    "Before applying the QR algorithm in practice, matrices are reduced to **Hessenberg form**: upper triangular plus one subdiagonal.\n",
    "\n",
    "$$\\mathbf{H} = \\begin{bmatrix}\n",
    "* & * & * & * \\\\\n",
    "* & * & * & * \\\\\n",
    "0 & * & * & * \\\\\n",
    "0 & 0 & * & *\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**(a)** Using Householder reflections, reducing an $n \\times n$ matrix to Hessenberg form requires $O(n^3)$ operations. How many flops does one QR iteration require for:\n",
    "- A general $n \\times n$ matrix?\n",
    "- An $n \\times n$ Hessenberg matrix?\n",
    "\n",
    "(Hint: QR factorization of Hessenberg matrices can be done in $O(n^2)$ instead of $O(n^3)$.)\n",
    "\n",
    "**(b)** If you need to compute all eigenvalues and the QR algorithm converges in $k$ iterations, what is the total complexity of:\n",
    "- Direct QR on the original matrix: $O(kn^3)$\n",
    "- Hessenberg reduction + QR on Hessenberg form: $O(n^3 + kn^2)$\n",
    "\n",
    "**(c)** For $n = 1000$ and $k = 100$, estimate the speedup factor.\n",
    "\n",
    "### Problem 9: The Implicit Q Theorem\n",
    "\n",
    "The **Implicit Q Theorem** is foundational to understanding modern QR implementations.\n",
    "\n",
    "**Theorem:** Let $\\mathbf{A}$ be an $n \\times n$ matrix, and suppose $\\mathbf{Q} = [\\mathbf{q}_1, \\ldots, \\mathbf{q}_n]$ and $\\mathbf{Z} = [\\mathbf{z}_1, \\ldots, \\mathbf{z}_n]$ are orthogonal matrices such that:\n",
    "- Both $\\mathbf{Q}^T\\mathbf{A}\\mathbf{Q}$ and $\\mathbf{Z}^T\\mathbf{A}\\mathbf{Z}$ are in Hessenberg form\n",
    "- $\\mathbf{q}_1 = \\mathbf{z}_1$ (same first column)\n",
    "\n",
    "Then $\\mathbf{Q} = \\mathbf{Z}$ up to signs of columns.\n",
    "\n",
    "**(a)** Explain in your own words why this theorem matters for the QR algorithm. (Hint: it allows \"implicit\" computation of the QR factorization.)\n",
    "\n",
    "**(b)** In the shifted QR algorithm with shift $\\mu$, the first column of $\\mathbf{Q}$ in $(\\mathbf{A} - \\mu\\mathbf{I}) = \\mathbf{Q}\\mathbf{R}$ is determined by $\\mathbf{A}$ and $\\mu$. How does the implicit Q theorem allow us to perform a shifted QR step without explicitly forming $\\mathbf{A} - \\mu\\mathbf{I}$?\n",
    "\n",
    "**(c)** Why is Hessenberg form \"essentially unique\" given the first column of the similarity transformation?\n",
    "\n",
    "### Problem 10: Full Eigendecomposition via QR (Coding)\n",
    "\n",
    "The QR algorithm can compute both eigenvalues AND eigenvectors by tracking the accumulated orthogonal transformations.\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "Set Q_total = I (identity matrix)\n",
    "Set A_current = A\n",
    "For k = 1 to max_iterations:\n",
    "    Compute A_current = Q_k * R_k\n",
    "    Set A_current = R_k * Q_k\n",
    "    Update Q_total = Q_total * Q_k\n",
    "```\n",
    "\n",
    "At convergence, the columns of $\\mathbf{Q}_{\\text{total}}$ are approximate eigenvectors.\n",
    "\n",
    "**(a)** Implement this algorithm with the Rayleigh quotient shift.\n",
    "\n",
    "**(b)** Test on the matrix:\n",
    "\n",
    "```python\n",
    "C = np.array([\n",
    "    [6, 2, 1],\n",
    "    [2, 3, 1],\n",
    "    [1, 1, 1]\n",
    "], dtype=float)\n",
    "```\n",
    "\n",
    "**(c)** After convergence:\n",
    "- Extract eigenvalues from the diagonal of the final $\\mathbf{A}_k$\n",
    "- Extract eigenvectors as columns of $\\mathbf{Q}_{\\text{total}}$\n",
    "- Verify that $\\mathbf{A}\\mathbf{v}_i \\approx \\lambda_i \\mathbf{v}_i$ for each eigenpair\n",
    "\n",
    "**(d)** Compare your results with `scipy.linalg.eigh(C)`. Compute the error in eigenvalues and the error in eigenvector directions (use the angle between vectors).\n",
    "\n",
    "**(e)** How many iterations were required for the off-diagonal norm to drop below $10^{-10}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab1405-cd55-4e7f-b51d-a8d5c765a0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
