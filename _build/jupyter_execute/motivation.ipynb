{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cbb968-6901-4262-acaa-17be487fc200",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Linear algebra is one of the most well studied and widely applied branches of mathematics. In recent years, linear algebra has played a prominent role in the rise of machine learning and artificial intelligence through its key role in the development of deep neural networks such as those that power large language models such as ChatGPT, whose architectures can be loosely described simply as sequences of linear transformations interspersed with nonlinearities.\n",
    "\n",
    "The ubiquitious role of linear algebra in applications old and new means that there are plenty of resources - i.e. other books and/or online content - already available with which one can learn linear algebra; however, after spending more than a decade introducing students to linear algebra for the first time, I haven't found a resource that addresses the subject the way that I believe it should be addressed. So here we are.\n",
    "\n",
    "There are two primary challenges one faces when learning linear algebra at the level where it is usually introduced, around the second or third year of college and usually after a calculus course. The first is that even in an applied linear algebra course the subject is inherently more abstract than any of the math one would have learned previously. For example, where in calculus one considers functions 'one at a time', studying, say, properties of a function like $f(x) = xe^{-x}$ such as its extrema or end behavior, in linear algebra one considers infinite 'spaces' of vectors like $\\mathbb{R}^2$ and how such spaces are transformed under a given linear operator. This requires - or forces one to develop - some additional mathematical maturity. The second challenge is that even in an applied linear algebra course, interesting real-world applications are hard to come by until quite a bit of abstract theory has been developed. In my introductory applied linear algebra course it is often not until after mid-semester that we finally can begin to explore meaningful applications.\n",
    "\n",
    "Closely tied to both of these challenges is the computational complexity of linear algebra, a topic that is largely ignored in introductory courses but is central to every practical application of linear algebra. Every linear algebra operation is computationally expensive, and it is a disservice to a student of linear algebra to pretend otherwise. Linear systems are never solved via matrix inversion; it's simply to expensive and error-prone. Eigenvalues are not computed via determinants, determinants are too computationally expensive for anything other than toy examples and thought exercises. And yet, countless hours are spent teaching students techniques such as these that are never used in the real world. \n",
    "\n",
    "The purpose of this book is to put computation front and center in an introductory presentation of linear algebra. That is why it is written using Jupyter notebooks: so that examples can be coded up and computational costs considered. I hope you will leave with a realistic perspective on how linear algebra is done in the real world.  \n",
    "\n",
    "As is probably clear, this is an opinionated book that is not for everyone. One central opinion is that someone learning linear algebra needs to do just enough toy problems by hand to understand the operation or topic and then needs to consider the computational aspects of what they are doing: the computational costs, the stability of the operations, and so on. The presentation adheres to this approach throughout. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48527c02-ab2f-435b-bf0e-e2ea178b85e9",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "This book is a work in progress. Upon completion it will cover many of the usual topics covered in an introductory linear algebra course, including the complete solution of $A\\mathbf{x} = \\mathbf{b}$, eigenvalue and eigenvector calculations, and various matrix decompositions including $A=LU$, $A=QR$, and the singular value decomposition. Applications to be covered include the normal equations of linear regression, Markov chains, PCA, neural networks, and perhaps some additional topics from probability theory. A few standard topics that are not practical are omitted; for instance, determinants will be relegated to an appendix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1fed8d-0302-4d54-9fba-a63662b7bdbb",
   "metadata": {},
   "source": [
    "```{tableofcontents}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fa57b-5160-4c01-aff9-c154e2c84e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}