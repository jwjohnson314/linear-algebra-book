{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e427f4d-b002-4aca-9425-7f370fd9beab",
   "metadata": {},
   "source": [
    "# Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4dd57-ecc0-441c-8efe-fe43b6aa8d69",
   "metadata": {},
   "source": [
    "Every mathematical operation we perform entails *computational cost*. In order to perform the computation, we must invest time and computational resources. For small examples like those in the previous chapter, the computational costs are negligible, but most real-world use of linear algebra is not like that. At the time of this writing, widely used machine learning or AI tools perform extensive computations with matrices containing millions to billions of components. The computational costs of such operations are high, and computational cost must always be a consideration when actually using linear algebra; indeed, there are many important ideas and algorithms from linear algebra that simply aren't used because the costs are just too high. \n",
    "\n",
    "**Definition:** The *time complexity* of an algorithm is the number of elementary operations needed to perform the algorithm.\n",
    "\n",
    "The above definition has a number of implicit assumptions, the most important one for us being the assumption that each elementary operation takes an equal fixed amount of time.\n",
    "\n",
    "**Example:** The time complexity of adding two vectors of length $n$ is $n$, because adding the vectors requires $n$ elementary operations, one addition for each component of the new vector. \n",
    "\n",
    "**Example:** The time complexity of the dot product of two vectors of lenth $n$ is $2n-1$. There are $n$ multiplications to perform, followed by summing the values produced by the multiplication, of which there are $n$. Only $n-1$ additions are required to sum $n$ elements (check this with an example if it isn't obvious to you), therefore the time complexity of the dot product is $n + (n-1) = 2n-1$ as stated.\n",
    "\n",
    "In the abstract we often want to think about the complexity of an algorithm asymptotically; that is, what does the complexity look like for a 'large' number of inputs? When we consider 'large' $n$, it turns out that many of the details can be ignored, only the leading term matter. For example, if $n=10^6$, the time complexity of the dot product is $2\\times 10^6 -1 \\approx 10^6$ (maybe we can debate whether the 2 matters here, but the -1 certainly is negligible). For this reason we introduce the following notation.  \n",
    "\n",
    "**Notation:** Let $f(x)$ and $g(x)$ be functions defined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6026e2e-37fb-4332-87a4-651647649cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
