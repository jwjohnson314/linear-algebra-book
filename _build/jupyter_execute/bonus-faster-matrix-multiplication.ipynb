{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4145d66a-eabc-4fa3-a876-07eed442f9d6",
   "metadata": {},
   "source": [
    "# Optional: Faster Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c09b00-746c-4e22-a81a-e265cbc2a8a6",
   "metadata": {},
   "source": [
    "The row method of matrix multiplication is the standard approach used for calculating matrix products by hand. However, it is expensive: to multiply two dense $n\\times n$ matrices requires $n^2$ dot products be calculated, one for each of the $n^2$ entries in the result. Each dot product involves $n$ multiplications and $n-1$ additions, or $2n-1$ operations taken together. All told, then, multiplying two $n\\times n$ matrices requires $n^2(2n-1) = 2n^3 - n^2$ operations, or more succinctly, is $\\mathcal{O}(n^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001c656-8c2a-458d-8be8-782d11de57d8",
   "metadata": {},
   "source": [
    "It is worth noting that matrix multiplication is an *embarassingly parallel* problem; that is, every dot product that needs to be calculated can be calculated independently of the others, so the computation lends itself naturally to massive parallelization. This does require many threads ($n^2$, to be precise) and is the reason that NVIDIA's stock has gone up more than 30,000% over the past ten years: NVIDIA's H100 GPUs (state-of-the-art at the time of this writing) can execute over 250,000 concurrent threads, making them invaluable for speeding up the massive matrix multiplies required to train and perform inference with large language models and other state-of-the-art machine learning/AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af738c82-a368-4151-8443-464e738f1e5d",
   "metadata": {},
   "source": [
    "No matter how much compute is available, better algorithms are crucuial for getting the most out of one's hardware. There are many classic problems in  computer science that illustrate the impact that a good algorithm can have; consider for instance the [maximum subarray problem](https://en.wikipedia.org/wiki/Maximum_subarray_problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca4374-bcc4-41fa-b95a-82eafd59fc9f",
   "metadata": {},
   "source": [
    "```{admonition} The Maximum Subarray Problem\n",
    "\n",
    "Given integers $a_1$, $a_2, \\dots, a_n$, determine indices $i$ and $j$ such that $\\sum_{k=i}^j a_k$ is as large as possible. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d42691-b958-4125-9595-8c0f1a0abd24",
   "metadata": {},
   "source": [
    "**Example:** Given the values $-3, 2, 5, -4, 7, 2, -3, -6, 8$ and using 0-based indexing, $i=1$ and $j=5$ produce the sum $2 + 5 -4 + 7 + 2 = 12$, which is maximal. A naive exhaustive approach to this problem is $\\mathcal{O}(n^3)$, but a good algorithm (*Kadane's algorithm*) runs in $\\mathcal{O}(n)$, making the latter approach very quick even in situations where the naive algorithm is simply unrunnable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea9878-8d2f-4fbb-b8e6-6a0b996293fb",
   "metadata": {},
   "source": [
    "Although matrix multiplication was first described in 1812, it wasn't until 1969 that Volker Strassen showed that it could be done in less than $\\mathcal{O}(n^3)$ time, using an algorithm now named after him. Strassen's algorithm runs in $\\mathcal{O}(n^{\\log_27}) \\approx \\mathcal{O}(n^{2.8074})$. While this might seems a small improvement it is significant for large enough $n$, and Strassen's algorithm is currently implemented in widely-used linear algebra libraries such as LAPACK and BLAS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f76f3d-be5b-4434-bd56-16997a553d6c",
   "metadata": {},
   "source": [
    "Faster algorithm's than Strassen's exist, but they are generally not as numerically stable. This is a very active area of research. In January 2024, a [new algorithm](https://arxiv.org/abs/2307.07970) brought the complexity down to $\\mathcal{O}(n^{2.371552})$. It is theorized that the lower bound on the time needed to perform matrix multiplication is $n^{2 + \\mathcal{O}(1)}$, which would be remarkable if true, but this has not been proven and it is a major open question in theoretical computer science. The image below ([By Jochen Burghardt - Own work, CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=87408497)) shows the improvements in runtime made by various algorithms over the years along with a forecast of future gains.\n",
    "\n",
    "![mm improvements over time](assets/mm_complexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a585955c-0056-4be2-8d6c-d4eca04e2f56",
   "metadata": {},
   "source": [
    "## Strassen's Algorithm\n",
    "\n",
    "Strassens's algorithm is not at all intuitive. It is important to note at the outset that multiplication is generally a slower operation for a modern processor to perform than addition (generally 3-6 times slower; for this reason many compilers will replace $2x$ with $x + x$). Strassen noticed that when multiplying $2\\times2$ matrices it is possible to reduct the number of multiplications required from 8 using the standard approach to 7. This is done as follows: Let\n",
    "\n",
    "$$\n",
    "    A=\\begin{bmatrix}\n",
    "        a & b \\\\\n",
    "        c & d\n",
    "    \\end{bmatrix}\\text{ and }\n",
    "    B=\\begin{bmatrix}\n",
    "        e & f \\\\\n",
    "        g & h\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and define the following constants:\n",
    "\n",
    "- $m_1 = (a + d)\\times(e + h)$\n",
    "- $m_2 = (c + d)\\times e$\n",
    "- $m_3 = a\\times(f-h)$\n",
    "- $m_4 = d\\times(g - e)$\n",
    "- $m_5 = (a + b)\\times h$\n",
    "- $m_6 = (c - a)\\times(e + f)$\n",
    "- $m_7 = (b - d)\\times(g + h)$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "    AB = \\begin{bmatrix}\n",
    "            m_1 + m_4 - m_5 + m_7 & m_3 + m_5 \\\\\n",
    "            m_2 + m_4 & m_1 - m_2 + m_3 + m_6\n",
    "         \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4cba2-3188-45eb-bb47-bbd8eb4a46ec",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "            1 & 2 \\\\\n",
    "            -1 & 3\n",
    "        \\end{bmatrix}\\text{ and }\n",
    "        B=\\begin{bmatrix}\n",
    "            0 & 4 \\\\\n",
    "            -5 & 7\n",
    "        \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "- $m_1 = (1 + 3)\\times(0 + 7) = 28$\n",
    "- $m_2 = (-1 + 3)\\times0 = 0$\n",
    "- $m_3 = 1\\times(4 - 7)=-3$\n",
    "- $m_4 = 3\\times(-5 - 0) = -15$\n",
    "- $m_5 = (1 + 2)\\times7 = 21$\n",
    "- $m_6 = (-1 - 1)\\times(0 + 4) = -8$\n",
    "- $m_7 = (2 - 3)\\times(-5 + 7) = -2$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "    AB = \\begin{bmatrix}\n",
    "            28 + (-15) - 21 + (-2) & -3 + 21 \\\\\n",
    "            0  + (-15) & 28 - 0 + (-3) + (-8)\n",
    "        \\end{bmatrix} = \n",
    "        \\begin{bmatrix}\n",
    "            -10 & 18 \\\\\n",
    "            -15 & 17\n",
    "        \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "which you can verify with the conventional calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5f4e2-9fb5-4dbb-97c5-95eca92a47b3",
   "metadata": {},
   "source": [
    "To extend this to matrices with $n>2$, we need to introduce *block matrices*. This is simply a conceptualization of a matrix as a collection of compatible submatrices; for example, I could write a $4\\times4$ matrix $A$ using $2\\times2$ blocks:\n",
    "\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "            a_{11} & a_{12} & a_{13} & a_{14} \\\\\n",
    "            a_{21} & a_{22} & a_{23} & a_{24} \\\\\n",
    "            a_{31} & a_{32} & a_{33} & a_{34} \\\\\n",
    "            a_{41} & a_{42} & a_{43} & a_{44}\n",
    "        \\end{bmatrix}=\n",
    "        \\begin{bmatrix}\n",
    "            A_{11} & A_{12} \\\\\n",
    "            A_{21} & A_{22}\n",
    "        \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Here each $A_{ij}$ is a $2\\times2$ matrix; in particular, \n",
    "\n",
    "$$\n",
    "    A_{11}=\\begin{bmatrix}\n",
    "        a_{11} & a_{12} \\\\\n",
    "        a_{21} & a_{22}\n",
    "        \\end{bmatrix}\\text{ and }\n",
    "    A_{22} = \\begin{bmatrix}\n",
    "        a_{33} & a_{34} \\\\\n",
    "        a_{43} & a_{44}\n",
    "        \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "the components in the 'upper left corner' and 'lower right corner' of $A$ itself. Now, it is important to know that multiplication with compatible block matrices is the same as multiplication with ordinary matrices; for example,\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        A & B \\\\\n",
    "        C & D\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        E & F \\\\\n",
    "        G & H\n",
    "    \\end{bmatrix}=\n",
    "    \\begin{bmatrix}\n",
    "        AE + BG & AF + BH \\\\\n",
    "        CE + DG & CF + DH\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "using the standard row method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa7b93-673e-4684-9351-53d4e12aecb2",
   "metadata": {},
   "source": [
    "So, to extend Strassen's algorithm to matrices with $n>2$, we use recursion with block matrices:\n",
    "\n",
    "- Divide each $n\\times n$ matrix into four $(n/2)\\times(n/2)$ blocks as above\n",
    "- Apply the above 7-multiplication formula recursively\n",
    "- Combine the results using additions/subtractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311b4cd-15fa-490b-876a-e96b9048699f",
   "metadata": {},
   "source": [
    "A complete analysis of the runtime complexity of Strassen's algorithm is beyond the scope of this course (for now, I may add it later if there is interest). As you no doubt notices, there is a *significant* increase in additions for a very slight reduction in multiplications  (there are also additional space considerations that we are not considering here). For small matrices, this impacts the efficiency of the algorithm, and in practice Strassen's algorithm isn't usually used until $n$ is around 1000, where it begins to outperform other methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}