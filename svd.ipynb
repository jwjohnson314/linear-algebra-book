{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6378ad76-e130-4d0f-88e7-f0a934674476",
   "metadata": {},
   "source": [
    "# The Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa6744-d0f9-43c7-b536-b7b6eee1f9c8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Singular Value Decomposition or SVD is a matrix factorization that produces similar outputs to eigenvectors and eigenvalues, but can be calculated for general $m\\times n$ matrices where eigenvalues and eigenvectors are only defined for square matrices. There are numerous important applications that rely on the singular value decomposition as well as a number of deep theoretical insights that it provides. First, let's consider its structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f731c12-9202-4c06-81c3-bbaafa1dc8db",
   "metadata": {},
   "source": [
    "```{admonition} The Singular Value Decomposition\n",
    "\n",
    "The *singular value decomposition* or SVD of an $m\\times n$ real matrix $A$ is a factorization\n",
    "\n",
    "$$\n",
    "    A = U\\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $U$ is an orthogonal $m\\times m$ matrix$, $\\Sigma$ is an $m\\times n$ diagonal matrix, and $V$ is an $n\\times n$ orthogonal matrix. It is in general not unique.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb183410-8e37-42e8-8b08-fc3e621307ea",
   "metadata": {},
   "source": [
    "Proving that such a decomposition exists is relatively straightforward. \n",
    "\n",
    "**Proof**: Suppose that $A$ is an $m\\times n$ matrix of rank $r$. Recall that $A^TA$ is an $n\\times n$ symmetric matrix, so (by the Spectral Theorem) it is diagonalizable and its eigenvectors are orthogonal. Moreover, it is possible to show that all of the eigenvalues of $A^TA$ are greater than or equal to 0 (exercise!), so for $1\\leq i\\leq n$, we can set $\\sigma_i = \\sqrt{\\lambda_i}$. Without loss of generality, assume $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_n$, and order the associated unit eigenvectors $\\mathbf{v}_1, \\mathbf{v}_2,\\dots,\\mathbf{v}_n$ as the columns of an orthogonal matrix $V$ analogously. \n",
    "\n",
    "Now for $1\\leq i\\leq r$, define $\\mathbf{u}_i = \\frac{1}{\\sigma_i}A\\mathbf{v}_i$. The $\\mathbf{u}_i's$ are orthonormal unit vectors (exercise!), and if $r \\leq m$, we can use Gram-Schmidt to extend the set $\\mathbf{u}_1,\\dots,\\mathbf{u}_r$ to an orthonormal basis of $\\mathbb{R}^m$ and let $U$ be the matrix with columns $\\mathbf{u}_i$, $1\\leq i\\leq m$. \n",
    "\n",
    "Finally, define $\\Sigma \\in M_{m\\times n}(\\mathbb{R})$ as the $m\\times n$ matrix with $\\Sigma_{ii} = \\sigma_i$ for $1\\leq i\\leq \\text{min}(m, n)$. \n",
    "\n",
    "To verify that this produces the desired decomposition, observe that by defining $\\mathbf{u}_i$ as above, we can write $AV = U\\Sigma$, and because $V$ is orthogonal, $A = U\\Sigma V^T$. $\\blacksquare$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c7ca7-a1af-470a-9936-3ff07532832f",
   "metadata": {},
   "source": [
    "## Computation of the SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725cb4f-531c-44cd-a110-b8fb793aa787",
   "metadata": {},
   "source": [
    "Computation of the full SVD is expensive. We will not do any computations by hand here; instead, we will describe the process as it is usually implemented in software. Typically, the SVD is calculated in two steps. First, the matrix under consideration is reduced to a bidiagonal matrix $B$. A bidiagonal matrix is a band matrix whose only nonzero entries are on the diagonal and either immediately above or immediately below the diagonal. Second, an eigenvalue approximation algorithm such as the $QR$-algorithm is applied to $B^TB$ to obtain the eigenvalues and eigenvectors of $B^TB$ which can be used to construct $V$ and $\\Sigma$, and from these the $\\mathbf{u}_i$'s can be calculated using the formula above.\n",
    "\n",
    "Assuming $m\\geq n$, the reduction of a matrix to bidiagonal form is an $\\mathcal{O}(mn^2)$ operation (this is just Gaussian elimination, which is $\\mathcal{O}(n^3)$ when the matrix is $n\\times n$ and $\\mathcal{O}(mn^2)$ when the matrix is $m\\times n$), but once the matrix is in bidiagonal form, the second step is much less expensive: only $\\mathcal{O}(n)$, so that the computational complexity of the decomposition overall is $\\mathcal{O}(mn^2)$.\n",
    "\n",
    "In addition, in many (most?) practical applications, the full SVD is not required, and truncated or 'thin' singular value decompositions can be more efficiently calculated and stored. These are beyond the scope of this discussion, but are quite similar to the 'thin' QR decomposition previously discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fef11f-b4e0-4767-937a-a694c576cf03",
   "metadata": {},
   "source": [
    "## An Application of the SVD: Condition Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189e7d6-f53a-4d61-a0ff-fc786587087b",
   "metadata": {},
   "source": [
    "```{admonition} The Condition Number of a Matrix\n",
    "The *condition number* of a real matrix $A$ is written $\\kappa(A)$ and defined as\n",
    "\n",
    "$$\n",
    "    \\kappa(A) = \\frac{\\sigma_{max}}{\\sigma_{min}},\n",
    "$$\n",
    "\n",
    "where $\\sigma_{max}$ denotes the largest singular value of $A$ and $\\sigma_{min}$ denotes the smallest nonzero singular value of $A$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a07e2-152f-4a30-8078-5a7ba25142bb",
   "metadata": {},
   "source": [
    "If we think of singular values as playing a role analogous to eigenvalues for nonsquare matrices, we can view the condition number of a matrix as the ratio between the largest and the smallest 'stretches' that a matrix applies when it multiplies a vector. Intuitively, a condition number near 1 indicates that these stretches are about equal, and that is a good thing, as will be demonstrated below. Such a matrix is called *well-conditioned*. A larger condition number (*ill-conditioned*) indicates a matrix that is closer to being singular than a matrix with a smaller condition number, and $\\kappa(A) = \\infty$ if and only if $A$ is singular. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f31a54-5bdc-4c28-aa17-2a4b0913de08",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeef3626-8d74-4f1e-9b21-a2a6fd39a545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values of A: [2. 1.]\n",
      "Condition Number of A: 2.0\n",
      "Singular values of B: [2.00000050e+00 4.99999875e-07]\n",
      "Condition Number of B: 4000002.0003309543\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "\n",
    "A = np.array([[2, 0], [0, 1]])\n",
    "B = np.array([[1, 1], [1, 1.000001]])\n",
    "\n",
    "# calculate the singular values of A and B\n",
    "A_vals = la.svdvals(A)\n",
    "B_vals = la.svdvals(B)\n",
    "\n",
    "print(f'Singular values of A: {A_vals}')\n",
    "print(f'Condition Number of A: {A_vals.max() / A_vals.min()}')\n",
    "print(f'Singular values of B: {B_vals}')\n",
    "print(f'Condition Number of B: {B_vals.max() / B_vals.min()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8bff3-b395-4f87-a455-81b34e708c88",
   "metadata": {},
   "source": [
    "The example above is trivial but illustrates what the condition number is measuring: $A$ is a diagonal matrix with orthogonal rows/columns, and $\\kappa(A) = 2$. $B$, on the other hand, has rows/columns that are *almost* parallel. They are not exactly parallel, so $B$ is not singular, but it is very close to singular, and the large condition number of $4000002.0003309543$ reflects this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5993877-b5dc-4700-98cb-fe80fd76e18a",
   "metadata": {},
   "source": [
    "**Example, continued:** Suppose now we work with a vector that has some measurement error. That is, suppose the vector we should be working with is\n",
    "\n",
    "$$\n",
    "    \\mathbf{x} = \\begin{bmatrix}\n",
    "                    1 \\\\\n",
    "                    -1\n",
    "                \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "but because of some measurement error, we are actually working with \n",
    "\n",
    "$$\n",
    "    \\mathbf{x} = \\begin{bmatrix}\n",
    "                    1.001 \\\\\n",
    "                    -0.999\n",
    "                \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The error in measurement here is quite small; to quantify this, consider the ratio of the norm of the measured $\\mathbf{x}$ to the norm of the actual $\\mathbf{x}$. It is very close to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70d73d19-ccb9-4bbb-80cb-a6e1a646a7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0000004999998748)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_x = np.array([[1], [-1]])\n",
    "measured_x = np.array([[1.001], [-0.999]])\n",
    "\n",
    "la.norm(measured_x) / la.norm(true_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ed448-73e1-4fa9-b719-c3c770108cf6",
   "metadata": {},
   "source": [
    "But observe now how the error propagates via multiplication with a well-conditioned matrix compared to an ill-conditioned matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0357774a-9bfa-4300-b379-c6e965e72744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0006003198080637)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.norm(A @ measured_x) / la.norm(A @ true_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dedd164-981f-484e-adb4-e66e5b1ddb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2827.7208135379897)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.norm(B @ measured_x) / la.norm(B @ true_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf230bf-ab87-44ac-9d62-d03745cf97f5",
   "metadata": {},
   "source": [
    "For the well-conditioned matrix $A$, the error in the product is not much larger than the error in $\\mathbf{x}$ itself, but for the ill-conditioned matrix $B$, the error in the product is dramatically larger. \n",
    "\n",
    "The condition number of the data matrix $X$ is often considered when using linear regression: if $X$ is ill-conditioned we expect a 'brittle', unstable model where the model's predictions change dramatically with only slight changes in input, and measurement error can dramatically impact model performance. Equivalently, a high condition number is indicative of *multicollinearity* in the data; that is, data where there are correlations or linear dependencies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9e676-cb27-4014-8574-c3d3d7a156a0",
   "metadata": {},
   "source": [
    "## A Real-World Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd5be5-8c9e-4c27-8037-a893cb7773ea",
   "metadata": {},
   "source": [
    "Let's go back to the [Student Performance Dataset](https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) we looked at previously in the chapter on linear regression. Recall the dataset consists of 10,000 student records, with each record containing information about various predictors and a performance index.\n",
    "\n",
    "Predictor variables:\n",
    "\n",
    "- **Hours Studied:** The total number of hours spent studying by each student.\n",
    "- **Previous Scores:** The scores obtained by students in previous tests.\n",
    "- **Extracurricular Activities:** Whether the student participates in extracurricular activities (Yes or No).\n",
    "- **Sleep Hours:** The average number of hours of sleep the student had per day.\n",
    "- **Sample Question Papers Practiced:** The number of sample question papers the student practiced.\n",
    "\n",
    "Target Variable:\n",
    "\n",
    "- **Performance Index:** A measure of the overall performance of each student. The performance index represents the student's academic performance and has been rounded to the nearest integer. The index ranges from 10 to 100, with higher values indicating better performance.\n",
    "\n",
    "Let's look at how well-conditioned the data matrix is. First, we will preprocess the data as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63716a89-7b7c-46c1-82df-f0fc4af163d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv('./datasets/Student_Performance.csv')\n",
    "\n",
    "# one hot the categorical variables\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# add the intercept column of 1's\n",
    "df['Intercept'] = np.ones(shape=(10000,))\n",
    "\n",
    "# drop the target and the redundant Extracurricular column\n",
    "df = df.drop(['Extracurricular Activities_No', 'Performance Index'], axis=1)  \n",
    "\n",
    "# convert to NumPy array\n",
    "X = df.values.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faaca09f-128d-42b8-ac94-7f7321875c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451.7820193598499\n"
     ]
    }
   ],
   "source": [
    "X_vals = la.svdvals(X)\n",
    "\n",
    "print(X_vals.max() / X_vals.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde42d3-21b2-47d0-8cb1-b224177cb55f",
   "metadata": {},
   "source": [
    "This is a relatively large condition number, but nowhere near our very ill-conditioned $2\\times 2$ example. We should expect a linear regression model here to exhibit some instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b9d78e-2b5d-4369-a608-624643cadf92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
