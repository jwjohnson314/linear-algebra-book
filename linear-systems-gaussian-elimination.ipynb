{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf402ead-3c84-487c-9218-b1dc3d2610b9",
   "metadata": {},
   "source": [
    "# Linear Systems and Gaussian Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3e01e-3d0a-41c0-9844-d239d4e4bc31",
   "metadata": {},
   "source": [
    "## The Elimination Method\n",
    "It is common to first encounter matrices as a tool for solving linear systems of equations by hand. At a basic level, matrices serve as a shorthand notation that simplify the process of adding or subtracting scalar multiples of equations to eliminate variables, a process called the *elimination method* of solving linear systems. The following example illustrates that approach and how we can use matrices to track the relevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86abee83-e181-478e-94c9-f6bc33e344be",
   "metadata": {},
   "source": [
    "**Example:** Solve the linear system below:\n",
    "\n",
    "$$\n",
    "    \\begin{alignat}\n",
    "        2x + y &= 4 \\\\\n",
    "        4x -2y &= 8.\n",
    "    \\end{alignat}\n",
    "$$\n",
    "\n",
    "The elimination method is defined by three 'legal' operations that can be performed with the equations in a linear system, with the goal of producing a simpler equation than any currently in the system that nevertheless shares the solution. The three operations are given below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a62a5a-c3a8-4b17-b82d-794e383f02c4",
   "metadata": {},
   "source": [
    "```{admonition} Elimination Operations\n",
    ":class: important\n",
    "The following three operations on a linear system of equations dd not change the solution to the system.\n",
    "\n",
    "- Changing the order in which the equations are written down,\n",
    "- Multiplying any equation in the system by a nonzero scalar,\n",
    "- Adding or subtracting any two equations in the system.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f31d13-0dc7-4e40-b89f-0470528dff70",
   "metadata": {},
   "source": [
    "Combining the last two operations in particular is key, because we can use them to produce simpler equations; specifically equations with fewer unknowns than any equation currently in the system. Here, for instance, we can take the first equation in the system and multiply it by 2: \n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        2\\cdot(2x + y) &= 2\\cdot 4 \\\\\n",
    "        4x -2y &= 8,\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        4x +2y &= 8 \\\\\n",
    "        4x -2y &= 8.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "Then we can add the result to the second equation to produce:\n",
    "\n",
    "$$\n",
    "    8x = 16,\n",
    "$$\n",
    "\n",
    "so $x = 2$. Then if we go back to the first equation and substitute $x=2$ into it, we see $2\\cdot2 + y = 4$ or $4 + y = 4$, which implies that $y=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5e432-48c6-4dfa-8883-ae8382a737f0",
   "metadata": {},
   "source": [
    "## Gaussian Elimination\n",
    "\n",
    "All of the arithmetic in the steps above is performed on the coefficients and constants in the system. The variables serve only as placeholders, and as such we can organize the data that defines the system into an array that we can manipulate more efficiently and effectively, particularly when the system is larger than this toy example:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 4 \\\\\n",
    "        4 & -2 & | & 8\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is called the *augmented matrix* of the linear system. Each row represents an equation, and each column contains the coefficients of one of the variables, aside from the last column, which contains the constants in the sytem. The vertical bar is not necessary, but is helpful to visually separate the column of constants from the columns of coefficients.\n",
    "\n",
    "Now let's translate the 'legal' steps of the elimination method to operations on the augmented matrix of the system. These operations are called *row operations*:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93c2b5-4996-4049-ab7b-d44f3d733de0",
   "metadata": {},
   "source": [
    "```{admonition} Row Operations on an Augmented Matrix\n",
    ":class: important\n",
    "\n",
    "The following operations on the rows of an augmented matrix do not change the solution of the underlying linear system.\n",
    "\n",
    "- Any two rows of the matrix can be interchanged,\n",
    "- Any row can be multiplied by a nonzero constant, and\n",
    "- Any two rows can be added or subtracted.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69abe8b6-db03-4894-b95e-8e8af932ee5c",
   "metadata": {},
   "source": [
    "Note that these are just the elimination operations expressed in terms of the augmented matrix that represents the underlying linear system.\n",
    "\n",
    "**Example:** Let's solve the linear system \n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        2x +y &= 6 \\\\\n",
    "        4x -2y &= 8\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "using row operations on the augmented matrix of the system. First, form the matrix as above:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 6 \\\\\n",
    "        4 & -2 & | & 8\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Now, let's combine two row operations into one: we will multiply the first row by 2 and then add the result to row 2. We will put the row that results from these two operations into the augmented matrix in place of row 2, since it is just a 'legally' altered version of row 2:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 6 \\\\\n",
    "        4 & -2 & | & 8\n",
    "    \\end{bmatrix} \\overset{R_2 + 2R_1\\to R_2}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 6 \\\\\n",
    "        8 & 0 & | & 20\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Now consider the second row of the augmented matrix: it represents the equation $8x + 0y = 20$ or just $8x=20$, so we can see that $x=20/8=5/2$. Suppose we now multiply the second equation by $1/4$ and subtract the result from row 1:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 6 \\\\\n",
    "        4 & -2 & | & 8\n",
    "    \\end{bmatrix} \\overset{R_2 + 2R_1\\to R_2}{\\sim}\n",
    "        \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 6 \\\\\n",
    "        8 & 0 & | & 20\n",
    "    \\end{bmatrix} \\overset{R_1 - 1/4R_2 \\to R_1}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        0 & \\hfill1 & | & 1 \\\\\n",
    "        8 & 0 & | & 20\n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and now the first row represents the equation $0x + y = 1$, or simply $y=1$. Let's add two more row operations: a row swap, and a scalar multiply:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 6 \\\\\n",
    "        4 & -2 & | & 8\n",
    "    \\end{bmatrix} \\overset{R_2 + 2R_1\\to R_2}{\\sim}\n",
    "        \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 6 \\\\\n",
    "        8 & 0 & | & 20\n",
    "    \\end{bmatrix} \\overset{R_1 - 1/4R_2 \\to R_1}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        0 & \\hfill1 & | & 1 \\\\\n",
    "        8 & 0 & | & 20\n",
    "    \\end{bmatrix} \\overset{R_1 \\leftrightarrow R_2}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        8 & 0 & | & 20 \\\\\n",
    "        0 & 1 & | & 1\n",
    "    \\end{bmatrix} \\overset{1/8 R_1 \\to R_1}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & | & 5/2 \\\\\n",
    "        0 & 1 & | & 1\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "With those last two row operations, we've produced an augmented matrix corresponding to the much simpler linear system\n",
    "\n",
    "$$ \n",
    "    \\begin{align}\n",
    "        x &= 5/2 \\\\\n",
    "        y &= 1\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "and this is the solution to our original system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7968f09-fb58-47a1-8d1f-2313b193d86f",
   "metadata": {},
   "source": [
    "```{admonition} Notation\n",
    ":class: important\n",
    "\n",
    "We use the $\\sim$ symbol to indicate row operations because we are changing the augmented matrix, and the augmented matrix before the row operation is not equal to the augmented matrix after the row operation. It does, however, contain a linear system with the same solution. The symbols above the $\\sim$ indicate precisely what row operations are taking place; for example, $R_1 + 2R_2 \\to R_1$ can be read 'multiply row 2 by 2, add the result to row 1, and store the result in row 1. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63ccee-8d9f-45c3-8386-0cc9bd4acdc3",
   "metadata": {},
   "source": [
    "Using an augmented matrix to solve a system like the one in the previous example is overkill; the value of the augmented matrix doesn't appear until we start working with systems of at least three equations in three or more unknowns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a55755-dec1-43b5-9861-02c755f80677",
   "metadata": {},
   "source": [
    "```{admonition} Definition\n",
    "The *main diagonal* of an $m\\times n$ matrix $A$ consists of the entries $a_{11}, a_{22},\\dots,a_{mm}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29abf0-0223-480f-bc2d-eb3143fa1466",
   "metadata": {},
   "source": [
    "```{admonition} Important\n",
    ":class: important The key to solving a linear system using an augmented matrix is coming up with combinations of row operations to produce zeros off the main diagonal of the matrix. It's very important when doing so that we don't introduce nonzero values where we previously had zeros when we do this, or we may not be able to make progress toward solving the system. This isn't an issue for a $2\\times2$ system like the one in the previous example, but it is a crucial consideration when solving larger systems. For this reason, when we work with larger systems there are some additional rules of thumb that we will adopt. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48688410-2a1a-4f49-9706-d9945faa17d2",
   "metadata": {},
   "source": [
    "```{admonition} Row Operation Rules of Thumb\n",
    ":class: important\n",
    "\n",
    "- Make all entries below the main diagonal zero before making entries above the main diagonal zero.\n",
    "- When working below the main diagonal, work left to right; that is, make all entries (below the main diagonal) in column $j$ 0 before moving on to column $j+1$.\n",
    "- When working below the main diagonal in column $j$, work down; that is make entry $a_{ij}$ 0 before moving on to entry $a_{i+1j}$.\n",
    "- When working above the main diagonal, work right to left; that is, make all entries in column $j$ 0 before moving on to column $j-1$.\n",
    "- When working above the main diagonal in column $j$, work up; that is make entry $a_{ij}$ 0 before moving on to entry $a_{i-1j}$.\n",
    "- It's nice to have ones on the main diagonal (this will make row operations easier); swap rows or use scalar multiplication when convenient to produce these.\n",
    "\n",
    "Taken together, these heuristics suggest working in a counterclockwise around the augmented matrix performing row operations to produce zeros off of the main diagonal. This is called *Gaussian Elimination* and is illustrated in the following example.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3e883-0aea-4485-a8b3-2aed6d94fa88",
   "metadata": {},
   "source": [
    "**Example:** Use Gaussian elimination to solve the linear system\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        2x + 2y + z &= 3 \\\\\n",
    "        x + y + z &= 1 \\\\\n",
    "        3x -y -z &= 3\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "First, form the augmented matrix:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill2 & \\hfill1 & | & 3 \\\\\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        3 & -1 & -1 & | & 3\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, swap rows one and two to get a $1$ in position $1, 1$:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill2 & \\hfill1 & | & 3 \\\\\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        3 & -1 & -1 & | & 3\n",
    "    \\end{bmatrix} \\overset{R_1\\leftrightarrow R_2}{\\sim}\n",
    "        \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        2 & \\hfill2 & \\hfill1 & | & 3 \\\\\n",
    "        3 & -1 & -1 & | & 3\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We will work down the first column constructing row operations to produce zeros:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        2 & \\hfill2 & \\hfill1 & | & 3 \\\\\n",
    "        3 & -1 & -1 & | & 3\n",
    "    \\end{bmatrix} \\overset{R_2 -2R_1\\to R_2}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        0 & \\hfill0 & -1 & | & 1 \\\\\n",
    "        3 & -1 & -1 & | & 3\n",
    "    \\end{bmatrix} \\overset{R_3 - 3R_1\\to R_3}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        0 & \\hfill0 & -1 & | & 1 \\\\\n",
    "        0 & -4 & -4 & | & 0\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "At this point, we want to move to the second column and produce zeros below the main diagonal - that's really just in position $3, 2$. We only have two possible ways we could do this: a row operation that combines row 1 with row 3 and stores the result in row 3, or swapping rows 2 and 3. Aside from the fact that the second option is much simpler than the first, note also that if I construct *any* row operation that combines row 1 with row 3 the first entry of the result will necessarily be nonzero, because the first entry of row 1 is nonzero. This means that if I did this I would be destroying work that I already did. We don't want to do that! Let's swap the rows:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        0 & \\hfill0 & -1 & | & 1 \\\\\n",
    "        0 & -4 & -4 & | & 0\n",
    "    \\end{bmatrix} \\overset{R_2\\leftrightarrow R_1}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        0 & -4 & -4 & | & 0 \\\\\n",
    "        0 & \\hfill0 & -1 & | & 1\n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and then multiply row 3 by $-1$, because having 1's on the main diagonal is preferable.\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        0 & -4 & -4 & | & 0 \\\\\n",
    "        0 & \\hfill0 & -1 & | & 1\n",
    "    \\end{bmatrix} \\overset{-R_3\\to R_3}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        0 & -4 & -4 & | & 0 \\\\\n",
    "        0 & \\hfill0 & 1 & | & -1\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Now we work above the main diagonal, bottom to top, right to left:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        0 & -4 & -4 & | & 0 \\\\\n",
    "        0 & \\hfill0 & 1 & | & -1\n",
    "    \\end{bmatrix} \\overset{R_2 + 4R_3\\to R_2}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & \\hfill1 & | & 1 \\\\\n",
    "        0 & -4 & 0 & | & -4 \\\\\n",
    "        0 & \\hfill0 & 1 & | & -1\n",
    "    \\end{bmatrix} \\overset{R_1 - R_3\\to R_1}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill1 & 0 & | & 2 \\\\\n",
    "        0 & -4 & 0 & | & -4 \\\\\n",
    "        0 & \\hfill0 & 1 & | & -1\n",
    "    \\end{bmatrix} \\overset{-1/4R_2 \\to R_2}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        1 & 1 & 0 & | & \\hfill2 \\\\\n",
    "        0 & 1 & 0 & | & \\hfill1 \\\\\n",
    "        0 & 0 & 1 & | & -1\n",
    "    \\end{bmatrix} \\overset{R_1 - R_2 \\to R_1}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 & | & \\hfill1 \\\\\n",
    "        0 & 1 & 0 & | & \\hfill1 \\\\\n",
    "        0 & 0 & 1 & | & -1\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "At this point, the rows of the matrix give three simple equations that provide the solution to our system:\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        x &= \\hfill1 \\\\\n",
    "        y &= \\hfill1 \\\\\n",
    "        z &= -1\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb566a-3a71-4ba4-88ae-7a1cbb36ec87",
   "metadata": {},
   "source": [
    "## Computational Cost of Gaussian Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1631d9a-a77f-441a-8aac-063007cdc235",
   "metadata": {},
   "source": [
    "Gaussian Elimination is expensive. Assuming the coefficient matrix is $n\\times n$; following the algorithm as outlined above will result in $n^3/3 + n^2 - n/3$ multiplications/divisions and $n^3/3 + n^2/2 - 5n/6$ additions/subtractions. For large $n$ that's approximately $n^3/3$ or $\\mathcal{O}(n^3)$ operations. Working by hand on a $3\\times3$ example probably already feels a bit tedious; now consider how this scales:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab38a0b-67e6-4fa3-8641-6ac9faed1b76",
   "metadata": {},
   "source": [
    "| $n$ | # Multiplications/Divisions | # Additions/Subtractions | Total Operations |\n",
    "|-----|-----|-----|-----|\n",
    "| 3 | 17 | 11 | 28 |\n",
    "| 10 | 430 | 375 | 805 | \n",
    "| 100 | 343,300 | 338,250 | 681,550 | \n",
    "|1,000 | 334,333,000 | 333,832,500 | 668,165,500 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ec04b-6fb1-4b00-a96a-aa17d3533bf0",
   "metadata": {},
   "source": [
    "What this means is that we need to take steps to obtain efficiencies wherever possible. The remainder of this section will be devoted to that task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093e3a6-7fd6-4c43-983f-183e5402cd2b",
   "metadata": {},
   "source": [
    "## Row Reduction as Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6de17a-205c-4353-ae53-50f57df7e170",
   "metadata": {},
   "source": [
    "The reason that Gaussian elimination works to solve linear systems is that it is really a convenient way to express matrix multiplications. As we saw in the last section, a linear system can typically be expressed as a matrix equation $A\\mathbf{x} = \\mathbf{b}$; for example, the first system above \n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        2x + \\hfill y &= 4 \\\\\n",
    "        4x -2y &= 8.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "can be written as a matrix equation\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 \\\\\n",
    "        4 & -2\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x \\\\\n",
    "        y\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        4 \\\\\n",
    "        8\n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where the *coefficient matrix* is the square matrix on the left, we are solving for a vector of unknowns $\\mathbf{x}$ whose components are $x$ and $y$, and the constants from the right-hand side of the system are the result of multiplying $A$ with $\\mathbf{x}$ according to the way we defined matrix-vector multiplication previously.\n",
    "\n",
    "Now as a general rule, if I have an equation $A\\mathbf{x} = \\mathbf{b}$ and I multiply both sides of this equation by some matrix $E_{21}$, assuming that $E_{21}$ has the right dimensions for the multiplication to be performed, then equality must still hold: $E_{21}A\\mathbf{x} = E_{21}\\mathbf{b}$. Let's look at this with a specific $E_{21}$:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & 0 \\\\\n",
    "        -2 & 1 \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 \\\\\n",
    "        4 & -2\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x \\\\\n",
    "        y\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & 0 \\\\\n",
    "        -2 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        4 \\\\\n",
    "        8\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 \\\\\n",
    "        0 & -4\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x \\\\\n",
    "        y\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        4 \\\\\n",
    "        0\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Multiplying by this particular $E_{21}$ did the same thing as the specific row operation we would take to eliminate the entry in position $2,1$ of the augmented matrix of the system:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 4\\\\\n",
    "        4 & -2 & | & 8\n",
    "    \\end{bmatrix} \\overset{R_2 - 2R_1 \\to R_2}{\\sim}\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 & | & 4\\\\\n",
    "        0 & -4 & | & 0\n",
    "    \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f724f959-1f44-4740-af9c-6ea229ebb08e",
   "metadata": {},
   "source": [
    "The matrix $E_{21}$ above is called an *elimination matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e79ce09-29b6-4a5f-92db-fd3ae8121639",
   "metadata": {},
   "source": [
    "```{admonition} Definition\n",
    "An *elimination matrix* is a square matrix with ones on the main diagonal and a single nonzero entry off of the main diagonal. We use the notation $E_{ij}$ to indicate an elimination matrix with a nonzero entry in position $i, j$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0434f71-1d57-4c03-b6b6-f0097d58ed54",
   "metadata": {},
   "source": [
    "Elimination matrices are so called because as was demonstrated above, with the right choice of nonzero value they can be used to eliminate entries from other matrices. Row swaps come from *permutation matrices*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903fb03-f891-44fa-8250-4433403da3b5",
   "metadata": {},
   "source": [
    "```{admonition} Definition\n",
    "A *permutation matrix* is a square matrix in which each row and column contains only one nonzero entry, and that entry is 1. We use the notation $P_{ij}$ to denote a permutation matrix such that $P_{ij}A$ swaps rows $i$ and $j$ of $A$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca6f29-7484-4734-8572-6ec9165f5b34",
   "metadata": {},
   "source": [
    "For example, \n",
    "\n",
    "$$ \n",
    "    P_{13} = \\begin{bmatrix}\n",
    "        0 & 0 & 1 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        1 & 0 & 0\n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "can be used to swap rows 1 and 3 of a matrix; for example,\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        0 & 0 & 1 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        1 & 0 & 0\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        3 & -1 & 5 & \\hfill4 \\\\\n",
    "        0 & \\hfill2 & -2 & 1 \\\\\n",
    "        1 & \\hfill4 & \\hfill7 & 0 \n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        1 & \\hfill4 & \\hfill7 & 0  \\\\\n",
    "        0 & \\hfill2 & -2 & 1 \\\\\n",
    "        3 & -1 & \\hfill5 & 4\n",
    "    \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03b919-fd21-4f23-98fd-5e66117e9e4c",
   "metadata": {},
   "source": [
    "An alternative way to define a permutation matrix is as a matrix whose rows can be rearranged to produce the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4eb5e-0b11-4730-8635-e580d2be4d64",
   "metadata": {},
   "source": [
    "We can reverse the row operation performed by an elimination matrix by changing the sign of the off-diagonal entry; for instance, take the row-reduced coefficient matrix from the linear system above:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 \\\\\n",
    "        2 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 \\\\\n",
    "        0 & -4\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 \\\\\n",
    "        4 & -2\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The matrix on the left is almost the same $E_{21}$ from above, but with the sign of the entry in position $2, 1$ changed from positive to negative. Because this matrix performs the inverse of the operation that $E_{21}$ performs, we call it the inverse of $E_{21}$ and denote it $E_{21}^{-1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735da508-7893-4b78-aa58-4c94ab3a764c",
   "metadata": {},
   "source": [
    "Inverse matrices in general are important theoretical concepts but are computationally expensive to construct, so they aren't often used in practice. However, inverses for elimination matrices are an exception: they can be quickly constructed as just demonstrated. Note that $E_{21}^{-1}E_{21} = I$ and $E_{21}E_{21}^{-1} = I$; this is the key property of inverse matrices: if a matrix $A$ has an inverse $A^{-1}$, then $AA^{-1} = A^{-1}A = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0fc10-aad3-4a77-a3f8-163586e8b2bc",
   "metadata": {},
   "source": [
    "Moreover, inverses of elimination matrices serve an important use: they lead to a *factorization* of the coefficient matrix of a linear system called the $LU$-factorization that is widely used. Again, let's refer to the example above, where we have\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 \\\\\n",
    "        2 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 \\\\\n",
    "        0 & -4\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        2 & \\hfill1 \\\\\n",
    "        4 & -2\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In the product on the left side of the equals sign, the matrix on the left is a lower-triangular matrix ($L$), while the matrix on the right is an upper-triangular matrix ($U$). Multiplying them together produces the coefficient matrix of the linear system: $A=LU$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80e780-f928-421a-a5b4-79229bedd9a3",
   "metadata": {},
   "source": [
    "```{admonition} Important\n",
    ":class: important\n",
    "Although above we are using the coefficient matrix of the linear system in the example above for convenience, we could have taken any matrix, multiplied by an elimination matrix, inverted the elimination matrix, and obtained this factorization.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b32bbc-3899-4737-b45c-449b5a8cf347",
   "metadata": {},
   "source": [
    "## The $A=LU$ Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b778b16-1002-43ae-94d9-fabd79fdcf0e",
   "metadata": {},
   "source": [
    "```{admonition} Definition\n",
    "Let $A$ be a square matrix. An $LU$-factorization of $A$ is an expression of $A$ as a product of two square matrices $L$ and $U$ such that $A=LU$, $L$ is lower triangular, and $U$ is upper triangular. \n",
    "\n",
    "Not all square matrices have an $LU$-factorization, and when the factorization exist it is not unique.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3586215-210e-41c5-9613-f7340a970c29",
   "metadata": {},
   "source": [
    "**Example:** Suppose that\n",
    "\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "            \\hfill1 & 2 & 1 \\\\\n",
    "            \\hfill2 & 2 & 0 \\\\\n",
    "            -1 & 1 & 1\n",
    "        \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We will use elimination matrices to construct an $LU$-factorization of $A$. First, to eliminate the entry in position $2, 1$, we would multiply $A$ by an elimination matrix $E_{21}$:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill 1 & 0 & 0 \\\\\n",
    "        -2 & 1 & 0 \\\\\n",
    "        \\hfill 0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & 2 & 1 \\\\\n",
    "        \\hfill2 & 2 & 0 \\\\\n",
    "        -1 & 1 & 1\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & \\hfill2 & \\hfill1 \\\\\n",
    "        \\hfill0 & -2 & -2 \\\\\n",
    "        -1 & \\hfill1 & \\hfill1\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "        \n",
    "Now eliminate the entry in position $3, 1$ by multiplying with an appropriate $E_{31}$:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill 1 & 0 & 0 \\\\\n",
    "        -2 & 1 & 0 \\\\\n",
    "        \\hfill 0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & 2 & 1 \\\\\n",
    "        \\hfill2 & 2 & 0 \\\\\n",
    "        -1 & 1 & 1\n",
    "    \\end{bmatrix} = \n",
    "        \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & \\hfill2 & \\hfill1 \\\\\n",
    "        \\hfill0 & -2 & -2 \\\\\n",
    "        -1 & \\hfill1 & \\hfill1\n",
    "    \\end{bmatrix} = \n",
    "     \\begin{bmatrix}\n",
    "        \\hfill1 & \\hfill2 & \\hfill1 \\\\\n",
    "        \\hfill0 & -2 & -2 \\\\\n",
    "        0 & \\hfill3 & \\hfill2\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "And finally, the entry in position $3, 2$:\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        0 & 3/2 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill 1 & 0 & 0 \\\\\n",
    "        -2 & 1 & 0 \\\\\n",
    "        \\hfill 0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & 2 & 1 \\\\\n",
    "        \\hfill2 & 2 & 0 \\\\\n",
    "        -1 & 1 & 1\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        0 & 3/2 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        1 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & \\hfill2 & \\hfill1 \\\\\n",
    "        \\hfill0 & -2 & -2 \\\\\n",
    "        -1 & \\hfill1 & \\hfill1\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        0 & 3/2 & 1\n",
    "    \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "        \\hfill1 & \\hfill2 & \\hfill1 \\\\\n",
    "        \\hfill0 & -2 & -2 \\\\\n",
    "        0 & \\hfill3 & \\hfill2\n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & \\hfill2 & \\hfill1 \\\\\n",
    "        \\hfill0 & -2 & -2 \\\\\n",
    "        0 & \\hfill0 & -1\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The line above spells out the following computation: $E_{32}E_{31}E_{21}A = U$ (note that the matrix on the right is upper triangular). If we now invert each of the three row operations in reverse order, we will have our factorization: $A=E_{21}^{-1}E_{31}^{-1}E_{32}^{-1}U = LU$. The inversions are straightforward:\n",
    "\n",
    "$$\n",
    "    E_{32}^{-1} =  \n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        0 & -3/2 & 1\n",
    "    \\end{bmatrix}, E_{31}^{-1} = \n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & 0 & 0 \\\\\n",
    "        \\hfill0 & 1 & 0 \\\\\n",
    "        -1 & 0 & 1\n",
    "    \\end{bmatrix}, E_{21}^{-1} = \n",
    "    \\begin{bmatrix}\n",
    "        \\hfill 1 & 0 & 0 \\\\\n",
    "        2 & 1 & 0 \\\\\n",
    "        \\hfill 0 & 0 & 1\n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and the multiplication of elimination matrices is very quick:\n",
    "\n",
    "$$\n",
    "    E_{21}^{-1}E_{31}^{-1}E_{32}^{-1} = \n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        2 & 1 & 0 \\\\\n",
    "        -1 & -3/2 & 1\n",
    "    \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "which is clearly lower triangular. Finally, we can check that $A = LU$ for this $L$ and $U$:\n",
    "\n",
    "$$ LU = \n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 & 0 \\\\\n",
    "        2 & 1 & 0 \\\\\n",
    "        -1 & -3/2 & 1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & \\hfill2 & \\hfill1 \\\\\n",
    "        \\hfill0 & -2 & -2 \\\\\n",
    "        0 & \\hfill0 & -1\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "        \\hfill1 & 2 & 1 \\\\\n",
    "        \\hfill2 & 2 & 0 \\\\\n",
    "        -1 & 1 & 1\n",
    "    \\end{bmatrix} = A.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3f09c-30fa-4fbe-b03d-a0a2b7101514",
   "metadata": {},
   "source": [
    "## The Computational Efficiency of $A=LU$\n",
    "\n",
    "Calculating an $LU$ factorization is not generally faster than solving a system using elimination; as the $LU$ factorization is in effect just the elimination method expressed through matrices the computational cost is the same $\\mathcal{O}(n^3)$. The efficiency we gain from it shows up in a specific situation that comes up often in applications: when we have to solve $A\\mathbf{x} = \\mathbf{b}$ for one matrix $A$ but a large number of distinct vectors $\\mathbf{b}$. This is a quite common real-world scenario. Then, if we have a factorization $A=LU$, we can use the following steps to solve the systems:\n",
    "\n",
    "1. Set $U\\mathbf{x} = \\mathbf{y}$. This is a placeholder operation, so no computational cost.\n",
    "2. Solve $L\\mathbf{y} = \\mathbf{b}$ for $\\mathbf{y}$. The cost is $\\mathcal{O}(n^2)$, because $L$ is triangular.\n",
    "3. Solve $U\\mathbf{x} = \\mathbf{y}$ for $\\mathbf{x}$. The cost is $\\mathcal{O}(n^2)$, because $U$ is triangular.\n",
    "\n",
    "The computational cost of all of three steps taken together is $\\mathcal{O}(n^2)$. For small $n$, $\\mathcal{O}(n^3)$ and $\\mathcal{O}(n^2)$ are often similar in magnitude, but as $n$ grows the difference becomes dramatic, so it can be vastly more efficient to first obtain the $A=LU$ factorization at the $\\mathcal{O}(n^3)$ computational cost and then use it to solve $A\\mathbf{x} = \\mathbf{b}$ over and over using the steps above at the dramatically reduced $\\mathcal{O}(n^2)$ cost. \n",
    "\n",
    "A real-world example where this occurs is in structural engineering analysis of a building under different loading conditions. Consider analyzing a large building framework where:\n",
    "\n",
    "- the matrix $A$ represents the structural stiffness: each component is determined basaed on the geometry, material properties, and connections of the building,\n",
    "- $\\mathbf{x}$ represents the displacements/deflections at various nodes, and\n",
    "- $\\mathbf{b}$ represents the external forces applied to the structure.\n",
    "\n",
    "The engineer needs to solve the same structural system - so, the same $A$ - under many different loading scenarios:\n",
    "\n",
    "- Dead loads (permanent weight of structure)\n",
    "- Live loads (occupancy, furniture, equipment)\n",
    "- Wind loads from different directions\n",
    "- Snow loads\n",
    "- Seismic loads\n",
    "- Various combinations required by building codes\n",
    "\n",
    "Each loading case gives a different $\\mathbf{b}$, but the structural stiffness matrix A remains the same since the building geometry and materials haven't changed.\n",
    "\n",
    "Another real-world example that we will look at later comes up in marketing, where an analyst wants to predict a variety of different consumer behaviours which are captured in distinct vectors $\\mathbf{b}$ based on a fixed set of data about consumers which is captured in a single fixed matrix $A$. Again obtaining $A=LU$ is expensive, but once obtained can be used over and over for far less computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda0b01-0c70-4d2c-8d19-2bbd4613acde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
